# -*- coding: utf-8 -*-
"""kaggle_a_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fn1-sRnMlvjS-_g8ElGfHKjLnjuBitCC

for kaggle calculation
"/content/drive/My Drive/input/contest/"
"""

from google.colab import drive
drive.mount('/content/drive')

import gc
import os
import random
import csv
import sys
import json
import datetime
import time
from contextlib import redirect_stdout

import lightgbm as lgb
import numpy as np
import pandas as pd
import seaborn as sns
from collections import Counter
from numba import jit
pd.set_option('display.max_rows', 1000)
pd.set_option('display.max_columns', 1000)

from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import log_loss
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from sklearn import metrics
import statsmodels.api as sm
from statsmodels.tsa.arima_model import ARIMA
from fbprophet import Prophet
from tqdm import tqdm
import logging

logging.disable(logging.FATAL)

plt.style.use("seaborn")
sns.set(font_scale=1)

df = pd.read_csv("/content/drive/My Drive/input/m5-forecasting-accuracy/sales_train_evaluation.csv")
price = pd.read_csv("/content/drive/My Drive/input/m5-forecasting-accuracy/sell_prices.csv")
calendar = pd.read_csv("/content/drive/My Drive/input/m5-forecasting-accuracy/calendar.csv")

if os.path.exists("/content/drive/My Drive/submission.csv"):
    print("Read existing file.")
    submission = pd.read_csv("/content/drive/My Drive/submission.csv")
    # tr_rmse = np.loadtxt("/content/drive/My Drive/tr_rmse.txt")
    item_store_ids = list(df.id.unique())
    with open("/content/drive/My Drive/learned_id.txt") as f:
      learned_id = [s.strip() for s in f.readlines()]
    item_store_ids = list(set(item_store_ids) ^ set(learned_id))
else:
    print("Make a new file.")
    submission = pd.read_csv("/content/drive/My Drive/input/m5-forecasting-accuracy/sample_submission.csv")
    # tr_rmse = np.zeros(28)
    item_store_ids = list(df.id.unique())
    learned_id = []

df = pd.melt(df,id_vars = df.columns[df.columns.str.endswith("id")],value_vars = df.columns[df.columns.str.startswith("d_")])
df = df.rename(columns = {"value" : "sales"})

rand = 1024
nsplits = 5
params = {
"objective": "regression",
"boosting": "gbdt",
"max_depth": 5,
"num_leaves": 10,
"subsample": 0.8,
"subsample_freq": 1,
"bagging_seed": rand,
"learning_rate": 0.025,
"feature_fraction": 0.75,
"feature_seed": rand,
# "min_data_in_leaf": 200,
"lambda_l1": 20,
"lambda_l2": 2,
"random_state": rand,
"metric": "rmse"
}
folds = KFold(n_splits=nsplits)
categorical_features = ["event_name_1", "event_type_1", "event_name_2", "event_type_2", "wday", "month"]
df_fimp = pd.DataFrame()


# item_store_id = "HOBBIES_1_001_CA_1_validation"
for item_store_id in tqdm(item_store_ids):
    # filter one store_item_id
    df_store_item = df.loc[df.id == item_store_id].copy()
    df_future = df_store_item.head(28).copy()
    df_future.variable = [f"d_{i}" for i in range(1942,1970)]
    df_store_item = df_store_item.append(df_future)

    # merge with calendar and prices
    df_store_item = df_store_item.merge(calendar, left_on  = "variable", right_on = "d",how = "left")
    df_store_item = df_store_item.merge(price,on = ["store_id", "item_id", "wm_yr_wk"], how = "left")
    df_store_item = df_store_item.dropna(subset = ["sell_price"])

    # label encoder
    le = LabelEncoder()
    df_store_item.event_name_1 = le.fit_transform(df_store_item.event_name_1.astype(str))
    le = LabelEncoder()
    df_store_item.event_name_2 = le.fit_transform(df_store_item.event_name_2.astype(str))
    le = LabelEncoder()
    df_store_item.event_type_1 = le.fit_transform(df_store_item.event_type_1.astype(str))
    le = LabelEncoder()
    df_store_item.event_type_2 = le.fit_transform(df_store_item.event_type_2.astype(str))

 # rename
    state = df_store_item.state_id.iloc[0]
    df_store_item['date'] = pd.to_datetime(df_store_item['date'])
    df_p = df_store_item.copy().rename(columns = {"sales": "sales_0"}).loc[:,["d", "date", "wday", "month", "sales_0", "event_name_1","event_type_1","event_name_2","event_type_2",f"snap_{state}"]]

    # add features
    n_val = 180
    for i in range(1,n_val):
        df_next = df_store_item.copy()
        df_next.date = df_store_item.date+datetime.timedelta(days=i)
        df_next.loc[:,["date","sales" ]].head(10)
        df_p = df_p.merge(df_next.loc[:,["date","sales" ]],on = "date", how = "left").rename(columns = {"sales": f"sales_{i}"})
    df_p = df_p[n_val:].copy()

    # make train and test dataset
    X_train = df_p[:-28].copy()
    X_test = df_p.tail(28).copy()
    y_train = X_train.sales_0
    X_train.drop(["d", "date", "sales_0"], axis = 1, inplace = True)
    X_test.drop(["d", "date", "sales_0"], axis = 1, inplace = True)
    if len(X_train) < 10:
      continue

    for day_lag in range(1,29):
        if day_lag != 1:
            for i in range(day_lag-1,len(X_test)):
                X_test.iloc[i,i+8-day_lag] = pred
            X_train.loc[:,f"sales_{day_lag-1}"] = np.hstack((np.zeros(day_lag-1)+np.nan,pred_tr))[:-(day_lag-1)]
#             X_train.drop([f"sales_{day_lag-1}"], axis = 1, inplace = True)
#             X_test.drop([f"sales_{day_lag-1}"], axis = 1, inplace = True)
        X_te = X_test.iloc[(day_lag-1):day_lag].copy()
        pred = np.zeros(len(X_te)) 
        pred_tr = np.zeros(len(X_train))
        for tr_idx,va_idx in folds.split(X_train,y_train):
            X_half_1 = X_train.iloc[tr_idx]
            y_half_1 = y_train.iloc[tr_idx]
            X_half_2 = X_train.iloc[va_idx]
            y_half_2 = y_train.iloc[va_idx]

            with redirect_stdout(open(os.devnull, 'w')):
              d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=categorical_features, free_raw_data=False)
              d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=categorical_features, free_raw_data=False)
              model_half = lgb.train(params, train_set=d_half_1, num_boost_round=1000, valid_sets=[d_half_1,d_half_2], verbose_eval=100, early_stopping_rounds=50)

            pred += model_half.predict(X_te, num_iteration=model_half.best_iteration) / nsplits
            pred_tr += model_half.predict(X_train, num_iteration=model_half.best_iteration) / nsplits

            # tr_rmse[day_lag-1] += model_half.best_score['valid_1']['rmse']

#             df_fimp_1 = pd.DataFrame()
#             df_fimp_1["feature"] = X_train.columns.values
#             df_fimp_1["importance"] = model_half.feature_importance()
# 
#             df_fimp = pd.concat([df_fimp, df_fimp_1], axis=0)
            del model_half
            gc.collect()

        submission.loc[submission.id == item_store_id, f"F{day_lag}"] = pred
    submission.to_csv("/content/drive/My Drive/submission.csv", index=False)
    # np.savetxt("/content/drive/My Drive/tr_rmse.txt", tr_rmse)
    learned_id.append(item_store_id)
    with open("/content/drive/My Drive/learned_id.txt", mode='w') as f:
      f.writelines('\n'.join(learned_id))

print(tr_rmse/(nsplits * len(item_store_ids)))